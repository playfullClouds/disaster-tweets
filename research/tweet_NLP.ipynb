{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b12d7ed-ca2e-460f-8308-ded895970711",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b2338-f6b1-4bc5-a6bb-8eaf0216ca66",
   "metadata": {},
   "source": [
    "* Is the tweet about the disaster?\n",
    "* Does the tweet offer support for the victims of the disaster?\n",
    "* Does the tweet express any emotion to the victim of the disaster?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d87a6-b47f-4ae4-8214-3aa44a7f2a38",
   "metadata": {},
   "source": [
    "#### Import Necessary Liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2673d02-73d3-4518-a091-6c53c58b3e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harry\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from src.logger import log\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertTokenizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.cluster import KMeans\n",
    "# \n",
    "# from sklearn.decomposition import PCA\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81345c7-9c86-466b-8130-4910255dd029",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Convert the dataset to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81ac8db-96d1-4da0-bf0d-90d33819c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# harvei = 'C:\\disaster-tweets\\artifacts\\data_ingestion\\data\\harvey.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfffb81b-37a1-4596-9d8c-80ba4d0282c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# harvey = pd.read_table(harvei, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32652ff9-51eb-45a5-a04a-2dab8f1dacd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# harvey.to_csv('harvey.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0ffbe6-519c-4a02-b2a9-9edb9cd7c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#harvey.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829bf562-75eb-40d9-9d07-a1d2dac2da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal = \"\\\\Users\\\\harry\\\\Desktop\\\\Data_Science_BootCamp\\\\Data_science\\\\Projects\\\\NLP\\\\data\\\\california_wildfires_final_data.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ae418d-8991-401b-94b8-bbcb23ae645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cali = pd.read_table(cal, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67399b38-f453-4a7b-ac6b-fbc25d8ca824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cali.to_csv('cali.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3eacfa7-e5fd-47b8-a54f-1e8970d3e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cali.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "674682dd-13fd-4836-8f98-9b797fe0c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cali.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02837565-746d-44a5-8fbc-c3ea5e9aac82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eea8e75c-cb7d-41b0-bf34-b95ce7e8f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# irmi = '\\\\Users\\\\harry\\\\Desktop\\\\Data_Science_BootCamp\\\\Data_science\\\\Projects\\\\NLP\\\\data\\\\hurricane_irma_final_data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d3ad857-4c35-4315-9f0a-832a67d18589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# irma = pd.read_table(irmi, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df095a10-31b4-4e5d-a2c6-0e5baed5ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# irma.to_csv('irma.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58c5447f-cbad-4a70-a93b-fb64632d1e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#irma.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a9b8a-6549-4db4-98a3-5edff993e340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73293213-a3ed-4072-a7b2-2fb7ebf8ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mari = '\\\\Users\\\\harry\\\\Desktop\\\\Data_Science_BootCamp\\\\Data_science\\\\Projects\\\\NLP\\\\data\\\\hurricane_maria_final_data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1420e4b0-5e22-4a88-8e6d-9964834edc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maria = pd.read_table(mari, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b67d1c2d-577e-4d32-bab4-c8066fa204fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maria.to_csv(\"maria.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3d3f1d3-68c4-44c2-ac92-6bf7a465e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maria.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca4ff8-e823-440e-a9d5-4330d07f74f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71df62d6-d922-47b4-86c9-2c217d6fab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# irai = '\\\\Users\\\\harry\\\\Desktop\\\\Data_Science_BootCamp\\\\Data_science\\\\Projects\\\\NLP\\\\data\\\\iraq_iran_earthquake_final_data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1947dba8-e751-4ab0-bd2f-70c3eba33286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iraq = pd.read_table(irai, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb7b77cb-bddb-4b4a-addf-752aad36e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iraq.to_csv(\"iraq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c59bcd8-20ff-4a2f-bf4a-75c4fa060cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iraq.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b69b31-e5b8-4616-9c81-b6530f5a15cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2439f28f-e3f2-48f5-b1f8-b13777eab71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mexi = '\\\\Users\\\\harry\\\\Desktop\\\\Data_Science_BootCamp\\\\Data_science\\\\Projects\\\\NLP\\\\data\\\\mexico_earthquake_final_data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c5a12d6-7862-4086-92c1-0866ccf9b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mexico = pd.read_table(mexi, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "833112d3-3464-4eb4-ac4c-7cfd098963bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mexico.to_csv(\"mexico.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2030e738-77fd-434b-ace4-44f1aaf4f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mexico.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51bd28e-7ab2-4492-8c74-2c9e6c9a23a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07337af2-2e43-46ab-8631-af960e680d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sk = '\\\\Users\\\\harry\\\\Desktop\\\\Data_Science_BootCamp\\\\Data_science\\\\Projects\\\\NLP\\\\data\\\\srilanka_floods_final_data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f63c1dea-01a6-4be9-a759-382fe91734f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sri = pd.read_table(sk, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af9dd619-1bb6-4a87-8d8f-37eb24e5a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sri.to_csv(\"sri.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62cc8f3d-307f-4a4a-ab57-60424cf125ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sri.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3fdc80-e735-4309-af42-361ba5242692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "342b21de-d6d1-43a1-a8bb-68f5fc7bb94e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Combine all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb95bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\disaster-tweets\\artifacts\\data_ingestion\\tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42035284-688b-4cd2-9330-7508dbf76ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use concat instead of merge cos all data have the same column \n",
    "# df = pd.concat([cali, harvey, iraq, irma, maria, mexico, sri],\n",
    "#                 ignore_index = True,\n",
    "#                 sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58439b3e-da7a-4b71-86b0-b40fe48f4a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>text_info</th>\n",
       "      <th>text_info_conf</th>\n",
       "      <th>image_info</th>\n",
       "      <th>image_info_conf</th>\n",
       "      <th>text_human</th>\n",
       "      <th>text_human_conf</th>\n",
       "      <th>image_human</th>\n",
       "      <th>image_human_conf</th>\n",
       "      <th>image_damage</th>\n",
       "      <th>image_damage_conf</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>image_url</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917791044158185473</td>\n",
       "      <td>917791044158185473_0</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6766</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>1.0</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>0.6766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @Gizmodo: Wildfires raging through Northern...</td>\n",
       "      <td>http://pbs.twimg.com/media/DLyi_WYVYAApwNg.jpg</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>917791130590183424</td>\n",
       "      <td>917791130590183424_0</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>affected_individuals</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PHOTOS: Deadly wildfires rage in California ht...</td>\n",
       "      <td>http://pbs.twimg.com/media/DLymKm9UMAAu0qw.jpg</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id              image_id    text_info  text_info_conf  \\\n",
       "0  917791044158185473  917791044158185473_0  informative             1.0   \n",
       "1  917791130590183424  917791130590183424_0  informative             1.0   \n",
       "\n",
       "    image_info  image_info_conf                         text_human  \\\n",
       "0  informative           0.6766         other_relevant_information   \n",
       "1  informative           0.6667  infrastructure_and_utility_damage   \n",
       "\n",
       "   text_human_conf                 image_human  image_human_conf image_damage  \\\n",
       "0              1.0  other_relevant_information            0.6766          NaN   \n",
       "1              1.0        affected_individuals            0.6667          NaN   \n",
       "\n",
       "   image_damage_conf                                         tweet_text  \\\n",
       "0                NaN  RT @Gizmodo: Wildfires raging through Northern...   \n",
       "1                NaN  PHOTOS: Deadly wildfires rage in California ht...   \n",
       "\n",
       "                                        image_url  \\\n",
       "0  http://pbs.twimg.com/media/DLyi_WYVYAApwNg.jpg   \n",
       "1  http://pbs.twimg.com/media/DLymKm9UMAAu0qw.jpg   \n",
       "\n",
       "                                          image_path  \n",
       "0  data_image/california_wildfires/10_10_2017/917...  \n",
       "1  data_image/california_wildfires/10_10_2017/917...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81da02eb-4bf1-4089-816a-104aa22bf24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweet_id',\n",
       " 'image_id',\n",
       " 'text_info',\n",
       " 'text_info_conf',\n",
       " 'image_info',\n",
       " 'image_info_conf',\n",
       " 'text_human',\n",
       " 'text_human_conf',\n",
       " 'image_human',\n",
       " 'image_human_conf',\n",
       " 'image_damage',\n",
       " 'image_damage_conf',\n",
       " 'tweet_text',\n",
       " 'image_url',\n",
       " 'image_path']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the column names\n",
    "df.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93f93697-18f2-4d18-afe2-0f877ccd1038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18082, 15)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a11d2f6-dde7-4b96-be89-26ba555be669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18082 entries, 0 to 18081\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   tweet_id           18082 non-null  int64  \n",
      " 1   image_id           18082 non-null  object \n",
      " 2   text_info          18082 non-null  object \n",
      " 3   text_info_conf     18082 non-null  float64\n",
      " 4   image_info         18082 non-null  object \n",
      " 5   image_info_conf    18082 non-null  float64\n",
      " 6   text_human         18082 non-null  object \n",
      " 7   text_human_conf    18082 non-null  float64\n",
      " 8   image_human        18082 non-null  object \n",
      " 9   image_human_conf   18082 non-null  float64\n",
      " 10  image_damage       3627 non-null   object \n",
      " 11  image_damage_conf  3627 non-null   float64\n",
      " 12  tweet_text         18082 non-null  object \n",
      " 13  image_url          18082 non-null  object \n",
      " 14  image_path         18082 non-null  object \n",
      "dtypes: float64(5), int64(1), object(9)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fabfdf8c-1e39-43c2-8664-68cb82e358e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id               int64\n",
       "image_id              object\n",
       "text_info             object\n",
       "text_info_conf       float64\n",
       "image_info            object\n",
       "image_info_conf      float64\n",
       "text_human            object\n",
       "text_human_conf      float64\n",
       "image_human           object\n",
       "image_human_conf     float64\n",
       "image_damage          object\n",
       "image_damage_conf    float64\n",
       "tweet_text            object\n",
       "image_url             object\n",
       "image_path            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b93e8f9-14b0-4d15-ae81-199f8ba2977c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                 0\n",
       "image_id                 0\n",
       "text_info                0\n",
       "text_info_conf           0\n",
       "image_info               0\n",
       "image_info_conf          0\n",
       "text_human               0\n",
       "text_human_conf          0\n",
       "image_human              0\n",
       "image_human_conf         0\n",
       "image_damage         14455\n",
       "image_damage_conf    14455\n",
       "tweet_text               0\n",
       "image_url                0\n",
       "image_path               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "263e902a-8386-47b0-a1cd-62a72cb3ee2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <td>18082.0</td>\n",
       "      <td>9.105572e+17</td>\n",
       "      <td>1.129871e+16</td>\n",
       "      <td>8.699189e+17</td>\n",
       "      <td>9.069342e+17</td>\n",
       "      <td>9.101265e+17</td>\n",
       "      <td>9.160276e+17</td>\n",
       "      <td>9.320276e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_info_conf</th>\n",
       "      <td>18082.0</td>\n",
       "      <td>8.003146e-01</td>\n",
       "      <td>2.005088e-01</td>\n",
       "      <td>2.578000e-01</td>\n",
       "      <td>6.614000e-01</td>\n",
       "      <td>7.119000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_info_conf</th>\n",
       "      <td>18082.0</td>\n",
       "      <td>8.569490e-01</td>\n",
       "      <td>1.909750e-01</td>\n",
       "      <td>2.684000e-01</td>\n",
       "      <td>6.734000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_human_conf</th>\n",
       "      <td>18082.0</td>\n",
       "      <td>8.003146e-01</td>\n",
       "      <td>2.005088e-01</td>\n",
       "      <td>2.578000e-01</td>\n",
       "      <td>6.614000e-01</td>\n",
       "      <td>7.119000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_human_conf</th>\n",
       "      <td>18082.0</td>\n",
       "      <td>8.569490e-01</td>\n",
       "      <td>1.909750e-01</td>\n",
       "      <td>2.684000e-01</td>\n",
       "      <td>6.734000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_damage_conf</th>\n",
       "      <td>3627.0</td>\n",
       "      <td>7.890148e-01</td>\n",
       "      <td>2.059917e-01</td>\n",
       "      <td>2.752000e-01</td>\n",
       "      <td>6.606000e-01</td>\n",
       "      <td>7.027000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count          mean           std           min  \\\n",
       "tweet_id           18082.0  9.105572e+17  1.129871e+16  8.699189e+17   \n",
       "text_info_conf     18082.0  8.003146e-01  2.005088e-01  2.578000e-01   \n",
       "image_info_conf    18082.0  8.569490e-01  1.909750e-01  2.684000e-01   \n",
       "text_human_conf    18082.0  8.003146e-01  2.005088e-01  2.578000e-01   \n",
       "image_human_conf   18082.0  8.569490e-01  1.909750e-01  2.684000e-01   \n",
       "image_damage_conf   3627.0  7.890148e-01  2.059917e-01  2.752000e-01   \n",
       "\n",
       "                            25%           50%           75%           max  \n",
       "tweet_id           9.069342e+17  9.101265e+17  9.160276e+17  9.320276e+17  \n",
       "text_info_conf     6.614000e-01  7.119000e-01  1.000000e+00  1.000000e+00  \n",
       "image_info_conf    6.734000e-01  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "text_human_conf    6.614000e-01  7.119000e-01  1.000000e+00  1.000000e+00  \n",
       "image_human_conf   6.734000e-01  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "image_damage_conf  6.606000e-01  7.027000e-01  1.000000e+00  1.000000e+00  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892fda5c-a6a4-40f8-87d0-2f09d1cebe59",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea429c95-5008-4d29-9eee-04bd75ff85af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-21 23:49:48,520] 1053 - urllib3.connectionpool - DEBUG - connectionpool - Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\disaster-tweets\\dis\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-21 23:49:49,330] 547 - urllib3.connectionpool - DEBUG - connectionpool - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "975bd61d-f1fe-456f-8e6c-878fe00da22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function taken and modified \n",
    "# from https://stackoverflow.com/questions/54396405/how-can-i-preprocess-nlp-text-lowercase-remove-special-characters-remove-numb\n",
    "\n",
    "# def preprocess(sentence):\n",
    "#     #convert to strings series\n",
    "#     sentence=str(sentence)\n",
    "#     # lowercase everything\n",
    "#     sentence = sentence.lower()\n",
    "#     # remove html links and tags.\n",
    "#     # sentence=sentence.replace('{html}',\"\") \n",
    "#     sentence = sentence.encode('ascii', 'ignore').decode()\n",
    "#     sentence = re.sub(r'https*\\S+', ' ', sentence)\n",
    "#     sentence = re.sub(r'http*\\S+', ' ', sentence)\n",
    "    \n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     cleantext = re.sub(cleanr, '', sentence)\n",
    "    \n",
    "#     #remove unicode characters\n",
    "#     rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "#     # replace the numbers with space\n",
    "#     rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    \n",
    "#     #Regexp Tokenizer. Tokenise words while ignoring  punctuation.\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     tokens = tokenizer.tokenize(rem_num)  \n",
    "    \n",
    "# #     kens = nltk.word_tokenize(sentence)\n",
    "# #     sentence = nltk.Text(kens)\n",
    "# #     text_content = [''.join(re.split(\"[ .,;:!?‘’``''@#$%^_&*()<>{}~\\n\\t\\\\\\â-]\", word)) for word in sentence]\n",
    "    \n",
    "#     # remove stopwords if word is greater than 2\n",
    "#     filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    \n",
    "#     # Stemming the words\n",
    "#     stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    \n",
    "#     # Lemmatization\n",
    "#     lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    \n",
    "#     return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "# df['cleanText']=df['tweet_text'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f91af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3634bf4-bbc2-457b-9076-3d634945f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function taken and modified \n",
    "# from https://towardsdatascience.com/cleaning-text-data-with-python-b69b47b97b76\n",
    "\n",
    "# def text_preproc(x):\n",
    "#   # first we lowercase everything\n",
    "#     x = x.lower()\n",
    "#     x = ' '.join([word for word in x.split(' ') if len(word) >2 if not x in stoplist])\n",
    "#     # remove unicode characters\n",
    "#     x = x.encode('ascii', 'ignore').decode()\n",
    "#     x = re.sub(r'https*\\S+', ' ', x)\n",
    "#     x = re.sub(r'http*\\S+', ' ', x)\n",
    "#     # then use regex to remove @ symbols and hashtags\n",
    "#     x = re.sub(r'@\\S', '', x)\n",
    "#     x = re.sub(r'#\\S+', ' ', x)\n",
    "#     x = re.sub(r'\\'\\w+', '', x)\n",
    "#     x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "#     x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "#     x = re.sub(r'\\s{2,}', ' ', x)\n",
    "#     x = re.sub(r'\\s[^\\w\\s]\\s', '', x)\n",
    "#     # remove single letters and numbers surrounded by space\n",
    "#     x = re.sub(r'\\s[a-z]\\s|\\s[0-9]\\s', ' ', x)\n",
    "#     return x\n",
    "# df['clean_text'] = df.tweet_text.apply(text_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7777749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sentence):\n",
    "    \"\"\"Cleans a single sentence by removing unwanted tokens and applying normalization.\"\"\"\n",
    "        \n",
    "    # If the sentence is None or not a string, handle it gracefully\n",
    "    if not sentence or not isinstance(sentence, str):\n",
    "        return \"\"  # Return an empty string or apply other suitable logic\n",
    "    \n",
    "    sentence=str(sentence) #convert to strings series\n",
    "    sentence = sentence.lower()  # lowercase everything \n",
    "    sentence = re.sub(r'^rt[\\s]+', '', sentence) # Remove \"RT\" (retweet abbreviation)\n",
    "    sentence = re.sub(r'@\\w+', '', sentence) # Remove mentions (@username)\n",
    "    sentence = re.sub(r'https?://\\S+', '', sentence) # Remove URLs\n",
    "    sentence = re.sub(r'\\[.*?\\]', '', sentence) # remove square brackets\n",
    "    sentence = re.sub(r'<.*?>+', '', sentence) # remove angular brackets\n",
    "    sentence = re.sub(r'\\d+', '', sentence) #remove numbers\n",
    "    sentence = re.sub(r'\\w*\\d\\w*', '', sentence) #remove words containing numbers\n",
    "    sentence = re.sub(r'\\b(\\w)\\b', '', sentence)  # Remove single characters\n",
    "    \n",
    "    # Remove special characters and hashtags\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9\\s]', '', sentence)\n",
    "    sentence = re.sub(r'#', '', sentence)\n",
    "    \n",
    "    sentence = sentence.strip() # remove leading and trailing white spaces\n",
    "\n",
    "    # Tokenize sentence for BERT\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    sentence = ' '.join(tokens)\n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b129d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function to the tweet text column\n",
    "df['cleanText'] = df['tweet_text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff4d62a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-21 23:49:58,453] 6 - disaster-tweets - INFO - 1645174851 - Removed column: image_damage\n",
      "[2024-05-21 23:49:58,471] 6 - disaster-tweets - INFO - 1645174851 - Removed column: image_damage_conf\n"
     ]
    }
   ],
   "source": [
    " # Remove columns 'image_damage' and 'image_damage_conf' if they exist\n",
    "columns_to_drop = ['image_damage', 'image_damage_conf']\n",
    "for column in columns_to_drop:\n",
    "    if column in df.columns:\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "        log.info(f\"Removed column: {column}\")\n",
    "    else:\n",
    "        log.warning(f\"Column '{column}' not found in dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e21db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf9f9fc3-c82b-4d75-8969-7a06bae43026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RT @Gizmodo: Wildfires raging through Northern California are terrifying https://t.co/dI73RFzX2i https://t.co/k4KnvIimsU',\n",
       "       'PHOTOS: Deadly wildfires rage in California https://t.co/td9xT3vXOL https://t.co/OimwAncLew',\n",
       "       'RT @Cal_OES: PLS SHARE: Weâ€™re capturing wildfire response, recovery info here: https://t.co/r89LKpjLPj https://t.co/HiA1oQF2Ax',\n",
       "       ...,\n",
       "       \"@PaulMalignaggi Lol What u on about, Sergio Mora didn't have Horn winning https://t.co/OLvjcqEmLe\",\n",
       "       '#Cameroon : 4 #female suicide #Bombers kill 1 in Mora. https://t.co/XuBHLQXWVk https://t.co/AtDaoyO433',\n",
       "       'RT @saakey6251413: it happened during the visit of #CycloneMora on Bangladesh https://t.co/dw53n4O5Ve'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet_text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b31fa56c-4d4c-4447-9604-f6e728ba4ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wild ##fires raging through northern california are terrifying',\n",
       "       'photos deadly wild ##fires rage in california',\n",
       "       'pl ##s share were capturing wild ##fire response recovery info here',\n",
       "       ..., 'lo ##l what on about sergio mora didn have horn winning',\n",
       "       'cameroon female suicide bombers kill in mora',\n",
       "       'it happened during the visit of cyclone ##mora on bangladesh'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleanText'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c29baa2a-0db4-4ff1-ab53-5b4c6e421619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>text_info</th>\n",
       "      <th>text_info_conf</th>\n",
       "      <th>image_info</th>\n",
       "      <th>image_info_conf</th>\n",
       "      <th>text_human</th>\n",
       "      <th>text_human_conf</th>\n",
       "      <th>image_human</th>\n",
       "      <th>image_human_conf</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>image_url</th>\n",
       "      <th>image_path</th>\n",
       "      <th>cleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917791044158185473</td>\n",
       "      <td>917791044158185473_0</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6766</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>1.0</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>0.6766</td>\n",
       "      <td>RT @Gizmodo: Wildfires raging through Northern...</td>\n",
       "      <td>http://pbs.twimg.com/media/DLyi_WYVYAApwNg.jpg</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>wild ##fires raging through northern californi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>917791130590183424</td>\n",
       "      <td>917791130590183424_0</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>affected_individuals</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>PHOTOS: Deadly wildfires rage in California ht...</td>\n",
       "      <td>http://pbs.twimg.com/media/DLymKm9UMAAu0qw.jpg</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>photos deadly wild ##fires rage in california</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id              image_id    text_info  text_info_conf  \\\n",
       "0  917791044158185473  917791044158185473_0  informative             1.0   \n",
       "1  917791130590183424  917791130590183424_0  informative             1.0   \n",
       "\n",
       "    image_info  image_info_conf                         text_human  \\\n",
       "0  informative           0.6766         other_relevant_information   \n",
       "1  informative           0.6667  infrastructure_and_utility_damage   \n",
       "\n",
       "   text_human_conf                 image_human  image_human_conf  \\\n",
       "0              1.0  other_relevant_information            0.6766   \n",
       "1              1.0        affected_individuals            0.6667   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  RT @Gizmodo: Wildfires raging through Northern...   \n",
       "1  PHOTOS: Deadly wildfires rage in California ht...   \n",
       "\n",
       "                                        image_url  \\\n",
       "0  http://pbs.twimg.com/media/DLyi_WYVYAApwNg.jpg   \n",
       "1  http://pbs.twimg.com/media/DLymKm9UMAAu0qw.jpg   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  data_image/california_wildfires/10_10_2017/917...   \n",
       "1  data_image/california_wildfires/10_10_2017/917...   \n",
       "\n",
       "                                           cleanText  \n",
       "0  wild ##fires raging through northern californi...  \n",
       "1      photos deadly wild ##fires rage in california  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1b284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d21dc8f-92a4-4052-8985-3a9e07e65f66",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda3b49e-919f-4db3-bb33-442b1834cb56",
   "metadata": {},
   "source": [
    "**Word count for the raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75078c69-3a9c-4342-bce0-2ffd71c5ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def length(string):\n",
    "#     return len(string)\n",
    "# df['length'] = df['tweet_text'].apply(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ccb6fb6-13fe-4bbc-8d88-5687795d94d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,8))\n",
    "# bins = 150\n",
    "# plt.hist(df['length'], bins=bins)\n",
    "# plt.xlabel('length')\n",
    "# plt.ylabel('numbers')\n",
    "# # plt.legend(loc='upper right')\n",
    "# plt.xlim(0,150)\n",
    "# plt.grid()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb172b-154d-4330-9682-ee2aceb7dafa",
   "metadata": {},
   "source": [
    "**Word Frequent for clean data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39e0fc8e-466a-4ac0-9d56-7d9469db9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_length(string):\n",
    "#     return len(string)\n",
    "# df['clean_length'] = df['cleanText'].apply(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4874829d-aa55-4fb1-8c18-caadab72046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,8))\n",
    "# bins = 150\n",
    "# plt.hist(df['clean_length'], bins=bins)\n",
    "# plt.xlabel('length')\n",
    "# plt.ylabel('numbers')\n",
    "# # plt.legend(loc='upper right')\n",
    "# plt.xlim(0,150)\n",
    "# plt.grid()\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b59d70-11c1-4b3f-ad04-67868955aced",
   "metadata": {},
   "source": [
    "**Word Frequent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c67752a7-31c4-4646-948b-cdd5c5e24888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function taken and modified \n",
    "# # from https://www.kaggle.com/code/lucianfox/disaster-tweets-embeddings-ann\n",
    "\n",
    "# def word_freq_func(corpus):\n",
    "#     \"\"\"\n",
    "#     This function calculates frequency of unique tokens in the corpus\n",
    "#     Input   : corpus :<str> entire corpus of text\n",
    "    \n",
    "#     Returns : word_freq_df <pandas.dataframe> \n",
    "#     \"\"\"\n",
    "#     word_freq = dict()\n",
    "#     for token in corpus.split():\n",
    "#         if token not in word_freq.keys():\n",
    "#             word_freq[token]=1\n",
    "#         else:\n",
    "#             word_freq[token]+= 1\n",
    "        \n",
    "#     word_freq_df = pd.DataFrame({'words':word_freq.keys(),'values':word_freq.values()})\n",
    "#     word_freq_df = word_freq_df.sort_values(by='values',ascending=False)\n",
    "    \n",
    "#     return word_freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca560c9e-30c9-4ffa-beb9-2327bed08726",
   "metadata": {},
   "source": [
    "**Word Frequent for raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b66f26c9-e8b8-4b7c-8cba-adbd976f35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_list = df['tweet_text'].tolist()\n",
    "# corpus = \" \".join(corpus_list)\n",
    "# vocab = list(set(corpus.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b547a62a-d65a-4642-b8ef-7e77646055c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_freq_df = word_freq_func(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2eb74151-f420-45b8-bb82-d156e6cb9ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_df = word_freq_df.head(10)\n",
    "# freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc0ae7af-3878-4fd2-a4b1-92af73e3f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# color_list = ['Cyan', 'Orange', 'Green', 'Purple', 'Pink', 'Blue', 'Brown', 'Red', 'Grey', 'Olive' ]\n",
    "# graph = plt.bar(freq_df['words'], freq_df['values'], color = color_list)\n",
    "# plt.xticks(rotation=90, fontsize=10);\n",
    "# plt.title(\"Frequent word form the raw data\")\n",
    "# plt.xlabel(\"words\")\n",
    "# plt.ylabel(\"numbers\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8635d584-2a8f-4896-8f3b-6e5df52c1046",
   "metadata": {},
   "source": [
    "**Word Frequent for clean data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1ab1d12-25a5-42ce-96d4-bc340e7fd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_list = df['cleanText'].tolist()\n",
    "# corpus = \" \".join(corpus_list)\n",
    "# vocab = list(set(corpus.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42cbe250-7f1c-4315-a80f-1f82a49b51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_freq_clean_df = word_freq_func(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d7faebc-0c6a-441c-a127-4d69b825a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_tweet = word_freq_clean_df.head(10)\n",
    "# clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d9cbe27-38a3-4395-991a-ed5a0043d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "# plt.figure(figsize=(10,8))\n",
    "# color_list = ['Red', 'Orange', 'Blue', 'Purple', 'Pink', 'Green', 'Brown', 'Grey', 'Olive', 'Cyan']\n",
    "# graph = plt.bar(clean_tweet['words'], clean_tweet['values'], color = color_list)\n",
    "# plt.xticks(rotation=90, fontsize=10);\n",
    "# plt.title(\"Frequent word for clean data\")\n",
    "# plt.xlabel(\"words\")\n",
    "# plt.ylabel(\"numbers\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ef12c49-d8d5-46f1-a1a1-c4e206aa7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = df['text_info'].value_counts()\n",
    "# text\n",
    "\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.pie(text, labels = text, autopct='%1.1f%%')\n",
    "# # plt.axis('equal')\n",
    "# plt.title('Text Info')\n",
    "# plt.legend(labels=text.index);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb3ddb86-a69d-499b-b67f-5c547f9ad57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human = df['text_human'].value_counts()\n",
    "# human\n",
    "\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.pie(human, labels = human, autopct='%1.1f%%')\n",
    "# plt.axis('equal')\n",
    "# plt.title('Text Human')\n",
    "# plt.legend(labels=human.index);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d08cc64-b90d-438f-8ba1-9160552fbca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91527195-0942-408a-bba4-017c18f59280",
   "metadata": {},
   "source": [
    "**Word Cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4296ff72-ea65-4764-b208-ed0066ee0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,8))\n",
    "\n",
    "# wordcloud_ip = WordCloud(\n",
    "#                       background_color='black',\n",
    "#                       width=1800,\n",
    "#                       height=1400\n",
    "#                      ).generate(' '.join(df['cleanText']))\n",
    "# plt.grid(None)\n",
    "# plt.imshow(wordcloud_ip);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea82e46-2f80-4152-b0dd-28978ec59cc1",
   "metadata": {},
   "source": [
    "**Vactorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c480d3cb-ec09-4f2b-bf0d-4456f9ff3ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TfidVectorizer instance\n",
    "# vec = TfidfVectorizer(analyzer=preprocess)\n",
    "\n",
    "# #transform into a feature matrix\n",
    "# xp = vec.fit_transform(df['cleanText'])\n",
    "# # X = vec.fit_transform(df['cleanText'])\n",
    "\n",
    "# # X = pd.DataFrame.sparse.from_spmartrix(X)\n",
    "# xp = pd.SparseDataFrame()\n",
    "\n",
    "\n",
    "# # xp = pd.DataFrame.sparse.from_spmartrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b2d26f6-9aad-4c92-8e21-85e632dc4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(analyzer=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c28881f4-cf50-461c-99b3-e7744babb8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectorizer.fit_transform(df['cleanText']).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0caa1b01-2475-4eb1-a1f0-a24a68e54649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.DataFrame(vectorizer.fit_transform(df['cleanText']).todense())\n",
    "# X.columns = sorted(vectorizer.vocabulary_)\n",
    "# X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17299e70-ae67-4242-9459-8296fdfc6ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29de2e68-f181-4faf-a94e-56c5aa5ca4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(sublinear_tf= True, min_df=10, norm='l2', ngram_range=(1, 2), stop_words='english')\n",
    "# X = vectorizer.fit_transform(df['cleanText'])\n",
    "\n",
    "# pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2131c2-7414-45db-92f8-56bed207f785",
   "metadata": {},
   "source": [
    "#### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff3085-4413-46cf-9661-e6697e641168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "074bdcb2-0b5d-4742-ace3-3b438dd2fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code was taken and modified \n",
    "# # from https://towardsdatascience.com/text-clustering-using-k-means-ec19768aae48\n",
    "\n",
    "\n",
    "# # Using PCA to remove cols which has less co-relation\n",
    "# sklearn_pca = PCA(n_components = 2) \n",
    "\n",
    "# #fit_transform() is used to scale training data to learn parameters such as \n",
    "# # mean & variance of the features of training set and then these parameters are used to scale our testing data.\n",
    "# # As concluded using Elbow Method.\n",
    "# Y_sklearn = sklearn_pca.fit_transform(X.toarray())\n",
    "\n",
    "\n",
    "# k_clusters = 4\n",
    "\n",
    "\n",
    "# # Partition 'n' no. of observations into 'k' no. of clusters. \n",
    "# kmeans = KMeans(n_clusters= k_clusters, max_iter=600, algorithm = 'lloyd')\n",
    "\n",
    "# # Fitting k-means model  to feature array\n",
    "# fitted = kmeans.fit(Y_sklearn) \n",
    "\n",
    "# # predicting clusters class '0' or '1' corresponding to 'n' no. of observations\n",
    "# prediction = kmeans.predict(Y_sklearn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cce8a3-7a62-405c-8f4b-99c040be8aa2",
   "metadata": {},
   "source": [
    "**Elbow Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825afe35-25d1-4297-95c5-30f25f415247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f966a94-e6da-4e7c-9c16-860eb28da4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to get the number of clusters\n",
    "\n",
    "# code was taken and modified \n",
    "# from https://towardsdatascience.com/text-clustering-using-k-means-ec19768aae48\n",
    "\n",
    "# def elbow_method(Y_sklearn):\n",
    "#     \"\"\"\n",
    "#     This is the function used to get optimal number of clusters in order to feed to the k-means clustering algorithm.\n",
    "#     \"\"\"\n",
    "#     # Range of possible clusters that can be generated\n",
    "#     number_clusters = range(1, 10)\n",
    "    \n",
    "#     # Getting no. of clusters \n",
    "#     kmeans = [KMeans(n_clusters=i, max_iter = 600) for i in number_clusters] \n",
    "    \n",
    "#     # Getting score corresponding to each cluster.\n",
    "#     score = [kmeans[i].fit(Y_sklearn).score(Y_sklearn) for i in range(len(kmeans))] \n",
    "    \n",
    "#     # Getting list of positive scores.\n",
    "#     score = [i*-1 for i in score] \n",
    "    \n",
    "#     plt.plot(number_clusters, score)\n",
    "#     plt.xlabel('Number of Clusters')\n",
    "#     plt.ylabel('Score')\n",
    "#     plt.title('Elbow Method')\n",
    "#     plt.show();\n",
    "    \n",
    "# plt.figure(figsize=(10,6))\n",
    "# elbow_method(Y_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28576878-4f19-4b6f-a96c-eda5954ceab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e48df06-57e7-4181-b55d-83794244d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code was taken and modified \n",
    "# from https://towardsdatascience.com/text-clustering-using-k-means-ec19768aae48\n",
    "\n",
    "# def kmeans_clustering(Y_sklearn, fitted):\n",
    "#     \"\"\"\n",
    "#     This function will predict clusters on training set and plot the visuals of clusters as well.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Plotting scatter plot \n",
    "#     plt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction ,s=40, cmap='magma', linewidth = 5) \n",
    "    \n",
    "#     # It will give best possible coordinates of cluster center after fitting k-means\n",
    "#     centers = fitted.cluster_centers_ \n",
    "    \n",
    "#     plt.scatter(centers[:, 0], centers[:, 1],c='blue', s=200, alpha=0.6);\n",
    "    \n",
    "#     # As this can be seen from the figure, there is an outlier as well.\n",
    "# plt.figure(figsize=(10,6))\n",
    "# # plt.legend()\n",
    "# kmeans_clustering(Y_sklearn, fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6f565-a7a2-48d9-8d22-ace41eb50277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df89a3-8924-470e-b2b9-66326434ecd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "400c8e33-e0a7-4454-8e80-739f38134c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KMeans(n_clusters=k_clusters, init='k-means++', n_init=10, max_iter=600, tol=0.000001, random_state=42)\n",
    "# model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3b5adc81-46ea-46f9-a470-364c7e783b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "38a7ea44-9695-484d-825c-266f57123754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"ClusterName\"] = clusters\n",
    "# df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fe6bd-34e1-4efa-97f3-53ade044a979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61c103b6-345f-4dff-bb92-b30f2f8ee3bd",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3acd7af0-d8e9-40f6-b77f-dced624b83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataTransformationConfig:\n",
    "#     # source_dir = r'C:\\disaster-tweets\\artifacts\\data_cleaner\\cleaned_data_BERT.csv'  # Adjusted path\n",
    "#     destination_dir = r'C:\\disaster-tweets\\artifacts\\data_transformation'\n",
    "#     output_file_train = os.path.join(destination_dir, 'train_data.tfrecord')\n",
    "#     output_file_val = os.path.join(destination_dir, 'val_data.tfrecord')\n",
    "#     output_file_test = os.path.join(destination_dir, 'test_data.tfrecord')\n",
    "#     max_length = 128  # Max length for BERT tokenizer\n",
    "\n",
    "# config = DataTransformationConfig()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6cd69f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from C:\\disaster-tweets\\artifacts\\data_cleaner\\cleaned_data_BERT.csv\n",
      "Initial DataFrame shape: (18082, 14)\n"
     ]
    }
   ],
   "source": [
    "source_dir = r'C:\\disaster-tweets\\artifacts\\data_cleaner\\cleaned_data_BERT.csv'\n",
    "df = pd.read_csv(source_dir)\n",
    "print(f\"Loaded data from {source_dir}\")\n",
    "print(f\"Initial DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "87654b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure required columns exist\n",
    "assert 'cleanText' in df.columns, \"Input file does not contain required column 'cleanText'\"\n",
    "assert 'text_info' in df.columns, \"Input file does not contain required column 'text_info'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d44e8bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-21 23:50:01,290] 4 - disaster-tweets - INFO - 1530479374 - Initial DataFrame shape: (18082, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>text_info</th>\n",
       "      <th>text_info_conf</th>\n",
       "      <th>image_info</th>\n",
       "      <th>image_info_conf</th>\n",
       "      <th>text_human</th>\n",
       "      <th>text_human_conf</th>\n",
       "      <th>image_human</th>\n",
       "      <th>image_human_conf</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>image_url</th>\n",
       "      <th>image_path</th>\n",
       "      <th>cleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917791044158185473</td>\n",
       "      <td>917791044158185473_0</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6766</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>1.0</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>0.6766</td>\n",
       "      <td>RT @Gizmodo: Wildfires raging through Northern...</td>\n",
       "      <td>http://pbs.twimg.com/media/DLyi_WYVYAApwNg.jpg</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>wild ##fires raging through northern californi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>917791130590183424</td>\n",
       "      <td>917791130590183424_0</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>infrastructure_and_utility_damage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>affected_individuals</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>PHOTOS: Deadly wildfires rage in California ht...</td>\n",
       "      <td>http://pbs.twimg.com/media/DLymKm9UMAAu0qw.jpg</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>photos deadly wild ##fires rage in california</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id              image_id    text_info  text_info_conf  \\\n",
       "0  917791044158185473  917791044158185473_0  informative             1.0   \n",
       "1  917791130590183424  917791130590183424_0  informative             1.0   \n",
       "\n",
       "    image_info  image_info_conf                         text_human  \\\n",
       "0  informative           0.6766         other_relevant_information   \n",
       "1  informative           0.6667  infrastructure_and_utility_damage   \n",
       "\n",
       "   text_human_conf                 image_human  image_human_conf  \\\n",
       "0              1.0  other_relevant_information            0.6766   \n",
       "1              1.0        affected_individuals            0.6667   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  RT @Gizmodo: Wildfires raging through Northern...   \n",
       "1  PHOTOS: Deadly wildfires rage in California ht...   \n",
       "\n",
       "                                        image_url  \\\n",
       "0  http://pbs.twimg.com/media/DLyi_WYVYAApwNg.jpg   \n",
       "1  http://pbs.twimg.com/media/DLymKm9UMAAu0qw.jpg   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  data_image/california_wildfires/10_10_2017/917...   \n",
       "1  data_image/california_wildfires/10_10_2017/917...   \n",
       "\n",
       "                                           cleanText  \n",
       "0  wild ##fires raging through northern californi...  \n",
       "1      photos deadly wild ##fires rage in california  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "# df = pd.read_csv(config.source_dir)\n",
    "# log.info(f\"Loaded data from {config.source_dir}\")\n",
    "log.info(f\"Initial DataFrame shape: {df.shape}\")\n",
    "\n",
    "# Display the first few rows\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "af9bb50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_info\n",
       "informative        12855\n",
       "not_informative     5227\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_info'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4f7577b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text_info to binary labels\n",
    "df['target'] = df['text_info'].apply(lambda x: 1 if x == 'informative' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6d0a2490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into train, validation, and test sets\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Split the data into train, validation, and test sets using stratified splitting\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42, stratify=df['target'])  # 60% train, 40% temp\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['target'])  # 20% val, 20% test\n",
    "print(\"Dataset split into train, validation, and test sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "048af304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-21 23:50:01,616] 547 - urllib3.connectionpool - DEBUG - connectionpool - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\disaster-tweets\\dis\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 128  # Max length for BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "84aca00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training data before applying SMOTE\n",
    "def batch_tokenize_sentences(sentences, tokenizer, max_length):\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        sentences.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    return encoding['input_ids'], encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8cc9a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization\n",
    "train_texts = train_df['cleanText'].values\n",
    "train_targets = train_df['target'].values\n",
    "train_input_ids, train_attention_masks = batch_tokenize_sentences(train_texts, tokenizer, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cd057e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "train_input_ids_resampled, train_targets_resampled = smote.fit_resample(train_input_ids, train_targets)\n",
    "train_attention_masks_resampled, _ = smote.fit_resample(train_attention_masks, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1a074ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied SMOTE to the training data\n"
     ]
    }
   ],
   "source": [
    "# Convert the resampled texts and targets back to a DataFrame\n",
    "train_df_resampled = pd.DataFrame({\n",
    "    'input_ids': list(train_input_ids_resampled),\n",
    "    'attention_mask': list(train_attention_masks_resampled),\n",
    "    'target': train_targets_resampled\n",
    "})\n",
    "\n",
    "print(\"Applied SMOTE to the training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4a023d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize validation and test data\n",
    "val_texts = val_df['cleanText'].values\n",
    "test_texts = test_df['cleanText'].values\n",
    "\n",
    "val_input_ids, val_attention_masks = batch_tokenize_sentences(val_texts, tokenizer, max_length)\n",
    "test_input_ids, test_attention_masks = batch_tokenize_sentences(test_texts, tokenizer, max_length)\n",
    "\n",
    "val_df['input_ids'] = list(val_input_ids)\n",
    "val_df['attention_mask'] = list(val_attention_masks)\n",
    "test_df['input_ids'] = list(test_input_ids)\n",
    "test_df['attention_mask'] = list(test_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9042e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed train DataFrame shape: (15426, 3)\n",
      "Transformed val DataFrame shape: (3616, 3)\n",
      "Transformed test DataFrame shape: (3617, 3)\n"
     ]
    }
   ],
   "source": [
    "# Keep only the necessary columns\n",
    "train_df_resampled = train_df_resampled[['input_ids', 'attention_mask', 'target']]\n",
    "val_df = val_df[['input_ids', 'attention_mask', 'target']]\n",
    "test_df = test_df[['input_ids', 'attention_mask', 'target']]\n",
    "print(f\"Transformed train DataFrame shape: {train_df_resampled.shape}\")\n",
    "print(f\"Transformed val DataFrame shape: {val_df.shape}\")\n",
    "print(f\"Transformed test DataFrame shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f448f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Convert the data into TensorFlow datasets and save as TFRecord files\n",
    "def to_tf_dataset(dataframe):\n",
    "    input_ids = tf.ragged.constant(dataframe['input_ids'].tolist(), dtype=tf.int32)\n",
    "    attention_mask = tf.ragged.constant(dataframe['attention_mask'].tolist(), dtype=tf.int32)\n",
    "    target = tf.constant(dataframe['target'].tolist(), dtype=tf.int32)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_mask, target))\n",
    "    return dataset\n",
    "\n",
    "train_dataset = to_tf_dataset(train_df_resampled)\n",
    "val_dataset = to_tf_dataset(val_df)\n",
    "test_dataset = to_tf_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b93d61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to serialize example\n",
    "def _serialize_example(input_ids, attention_mask, target):\n",
    "    feature = {\n",
    "        'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=input_ids)),\n",
    "        'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=attention_mask)),\n",
    "        'target': tf.train.Feature(int64_list=tf.train.Int64List(value=[target])),\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d1ac3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to write TFRecord file\n",
    "def _write_tfrecord(dataset, filename):\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for input_ids, attention_mask, target in dataset:\n",
    "            example = _serialize_example(input_ids.numpy(), attention_mask.numpy(), target.numpy())\n",
    "            writer.write(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "85c52fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output file paths\n",
    "destination_dir = r\"C:\\disaster-tweets\\artifacts\\data_transformation\\new\"\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "output_file_train = os.path.join(destination_dir, 'train_data.tfrecord')\n",
    "output_file_val = os.path.join(destination_dir, 'val_data.tfrecord')\n",
    "output_file_test = os.path.join(destination_dir, 'test_data.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "93a2e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TensorFlow datasets as TFRecord files\n",
    "_write_tfrecord(train_dataset, output_file_train)\n",
    "_write_tfrecord(val_dataset, output_file_val)\n",
    "_write_tfrecord(test_dataset, output_file_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "72172249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved to C:\\disaster-tweets\\artifacts\\data_transformation\\new\\train_data.tfrecord, C:\\disaster-tweets\\artifacts\\data_transformation\\new\\val_data.tfrecord, C:\\disaster-tweets\\artifacts\\data_transformation\\new\\test_data.tfrecord\n"
     ]
    }
   ],
   "source": [
    "print(f\"Transformed data saved to {output_file_train}, {output_file_val}, {output_file_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec815f7",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ccf870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
